{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "SimpleSeq2Seq_translation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "n0YO0JD32A0c"
      ],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6Pbz2Cn2Azb",
        "colab_type": "text"
      },
      "source": [
        "#  Tutorial of Simple Seq2seq with Teacher forcing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2oEPwEz2Azc",
        "colab_type": "text"
      },
      "source": [
        "### 1) Read Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK-p19GY2Azd",
        "colab_type": "text"
      },
      "source": [
        "reference : https://www.tensorflow.org/tutorials/text/nmt_with_attention?hl=zh_cn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "cSXIGogG2Aze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import io\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fa6ziAP2Onk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "269f1ec8-0bf3-478f-cc5b-0727c16ec917"
      },
      "source": [
        "# %tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.2.0-rc4\n",
            "Running on TPU  ['10.80.83.210:8470']\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.80.83.210:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.80.83.210:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5YL90J_2Azj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "eb3157c7-fa47-47f4-cc98-ff314d5a0a01"
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN_yI6AA2Azl",
        "colab_type": "text"
      },
      "source": [
        "### 2) Text PreProcessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b742aPAH2Azm",
        "colab_type": "text"
      },
      "source": [
        "#### (1) Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpVq3d0T2Azm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 将 unicode 文件转换为 ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EXeIB0C2Azp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7xiir9S2Azr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    \n",
        "    sentence = unicode_to_ascii(sentence.lower().strip())\n",
        "    \n",
        "    sentence = \" \".join([contractions[word] if word in contractions else word for word in sentence.split(' ')])\n",
        "    \n",
        "    # 在单词与跟在其后的标点符号之间插入一个空格\n",
        "    # 例如： \"he is a boy.\" => \"he is a boy .\"\n",
        "    # reference：https://stackoverflosentence.com/questions/3645931/python-padding-punctuation-sentenceith-sentencehite-spaces-keeping-punctuation\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "\n",
        "    # 除了 (a-z, A-Z, \".\", \"?\", \"!\", \",\")，将所有字符替换为空格\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
        "\n",
        "    sentence = sentence.rstrip().strip()\n",
        "\n",
        "    # 给句子加上开始和结束标记\n",
        "    # 以便模型知道何时开始和结束预测\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26me8Hgi2Azx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "20139d3d-3e8b-4bf2-8eb5-5e6d71a6cad6"
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1a6neNR2Az0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. 去除重音符号\n",
        "# 2. 清理句子\n",
        "# 3. 返回这样格式的单词对：[ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')[:-1]\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFPryfeZ2Az4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en, sp = create_dataset(path_to_file, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtJQX7WZ2Az6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "de1af439-807f-44ae-f1ba-51d5898b6689"
      },
      "source": [
        "print(en[29000])\n",
        "print(sp[29000])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> tadpoles become frogs . <end>\n",
            "<start> los renacuajos se convierten en ranas . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4zvPgrM2Az9",
        "colab_type": "text"
      },
      "source": [
        "#### (2) sentence tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTN-mQmz2Az-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    \n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    \n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "    \n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gif9elXh2A0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    # 创建清理过的输入输出对\n",
        "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WrrJOYT2A0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 尝试实验不同大小的数据集\n",
        "num_examples = 30000\n",
        "input_tensor, output_tensor, input_tokenizer, output_tokenizer = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# 计算目标张量的最大长度 （max_length）\n",
        "max_length_targ, max_length_inp = max_length(output_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP4e_K1l2A0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length_output, max_length_input = max_length(output_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaD0o1Ls2A0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be406b01-1662-4b29-8df2-fc24f225e8e6"
      },
      "source": [
        "# 采用 80 - 20 的比例切分训练集和验证集\n",
        "input_tensor_train, input_tensor_val, output_tensor_train, output_tensor_val = train_test_split(input_tensor, output_tensor, test_size=0.2)\n",
        "\n",
        "# 显示长度\n",
        "print(len(input_tensor_train), len(output_tensor_train), len(input_tensor_val), len(output_tensor_val))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24000 24000 6000 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDEPJQVe2A0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uLEC6BI2A0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9dcb8762-7d01-4cf8-e707-c25403a71948"
      },
      "source": [
        "input_tensor_train[1000]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1, 350,  14,  16, 623,   3,   2,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-44O3bS2A0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "03e0a43e-fff1-4355-e741-26adf64907d2"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(input_tokenizer, input_tensor_train[7000])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(output_tokenizer, output_tensor_train[7000])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "25 ----> ella\n",
            "42 ----> tiene\n",
            "21 ----> una\n",
            "409 ----> cara\n",
            "1060 ----> bonita\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "26 ----> she\n",
            "50 ----> has\n",
            "9 ----> a\n",
            "213 ----> pretty\n",
            "439 ----> face\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0YO0JD32A0c",
        "colab_type": "text"
      },
      "source": [
        "#### (3) DataSet Creating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DygkmbLR2A0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2vOusyY2A0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, output_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmXDC10w2A0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e8b77244-6c4c-425a-9ff3-4c07a3f9ec6f"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkv91Nu12A0l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ab4f5bb2-73a7-4061-84cd-e71a2cf1239c"
      },
      "source": [
        "example_input_batch.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 16])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcpSUMD82A0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example_input_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7ZKn1UA2A0x",
        "colab_type": "text"
      },
      "source": [
        "### 2) Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBgOESmj2A0y",
        "colab_type": "text"
      },
      "source": [
        "##### 1) Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfgLjbma2A0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self,vocab_size,embedding_dim,encode_units):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.encode_units = encode_units\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_size,self.embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(units=self.encode_units,return_sequences=True,return_state=True)\n",
        "        \n",
        "    \n",
        "    def call(self,x):\n",
        "        encoder_embedding = self.embedding(x)\n",
        "        encode_output,encode_hidden_state = self.gru(encoder_embedding)\n",
        "        \n",
        "        return encode_output,encode_hidden_state\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpe9-8qv2A06",
        "colab_type": "text"
      },
      "source": [
        "##### 2) parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFAMy8RU2A08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_VOCAB_SIZE = len(input_tokenizer.word_index)+1\n",
        "ENCODER_EMBEDDING_SIZE = 256\n",
        "ENCODER_UNIT = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sayriMLR2A1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "05443ea3-1d93-40c2-dc86-a14b0556d929"
      },
      "source": [
        "INPUT_VOCAB_SIZE"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9414"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVBrE-nY2A1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(INPUT_VOCAB_SIZE,ENCODER_EMBEDDING_SIZE,ENCODER_UNIT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaPNm_sR2A1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encode_output,encode_hidden_state = encoder(example_input_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEOjxveG2A1H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "6bebb099-4a2b-4147-9c42-d33c2eb29139"
      },
      "source": [
        "encode_hidden_state"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
              "array([[-0.01884282,  0.00875622,  0.00219726, ...,  0.00774832,\n",
              "        -0.00805246, -0.01802043],\n",
              "       [-0.01860362,  0.00864717,  0.0023609 , ...,  0.00770958,\n",
              "        -0.00788831, -0.01813345],\n",
              "       [-0.01864812,  0.0086738 ,  0.00229547, ...,  0.00769006,\n",
              "        -0.00799331, -0.01801563],\n",
              "       ...,\n",
              "       [-0.01842104,  0.00855171,  0.00256057, ...,  0.00771517,\n",
              "        -0.00790944, -0.01803633],\n",
              "       [-0.01875985,  0.00869879,  0.0022138 , ...,  0.00773742,\n",
              "        -0.00802038, -0.01803016],\n",
              "       [-0.01880912,  0.00874791,  0.00221335, ...,  0.00775205,\n",
              "        -0.00801045, -0.01805654]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byAv1AVE2A1K",
        "colab_type": "text"
      },
      "source": [
        "### 3) Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtt-Wdbv2A1K",
        "colab_type": "text"
      },
      "source": [
        "##### 1) Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTMO9Bx32A1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,vocab_size,decode_unit,embedding_dim):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.decode_unit = decode_unit\n",
        "        self.embedding_dim = embedding_dim\n",
        "        \n",
        "        ### structure\n",
        "        self.gru =  tf.keras.layers.GRU(units=self.decode_unit,return_sequences=True,return_state=True)\n",
        "        \n",
        "        self.embeddding = tf.keras.layers.Embedding(self.vocab_size,self.embedding_dim)\n",
        "        \n",
        "        self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
        "        \n",
        "        \n",
        "        ### decode_input  【batch_size,word_index】\n",
        "    def call(self,decode_input,encode_output):\n",
        "        \n",
        "        decode_input = self.embeddding(decode_input)\n",
        "        \n",
        "        shape = (decode_input.shape[0],encode_output.shape[1]-decode_input.shape[1],decode_input.shape[2])\n",
        "        \n",
        "        padding = tf.zeros(shape)\n",
        "        \n",
        "        decode_input = tf.concat([decode_input,padding],axis = 1)\n",
        "        \n",
        "        concat_vector = tf.concat([encode_output,decode_input,decode_input],axis = -1)\n",
        "        \n",
        "        decode_output,decode_hidden_state = self.gru(concat_vector)\n",
        "        \n",
        "        decode_output = tf.reduce_sum(decode_output,axis = 1)\n",
        "        \n",
        "        y = self.fc(decode_output)\n",
        "        \n",
        "        return y\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzy1q9CE2A1N",
        "colab_type": "text"
      },
      "source": [
        "##### 2) Parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQum_rpv2A1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### parameter\n",
        "output_vocab_size = len(output_tokenizer.word_index)+1\n",
        "\n",
        "DECODER_UNIT = ENCODER_UNIT\n",
        "\n",
        "encode_embedding_dim = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCMvOF082A1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "41955b54-5359-442c-f4f2-928969a589d8"
      },
      "source": [
        "output_vocab_size"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4921"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys4Uwz7y2A1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = Decoder(output_vocab_size,DECODER_UNIT,encode_embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ-DV82p2A1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decode_input = example_target_batch[:, :1]\n",
        "# decode_input = tf.convert_to_tensor([output_tokenizer.word_index['<start>']] * BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXSnE5Sv2A1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decode_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZNFoAkS2A1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = decoder(decode_input,encode_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrtQNWTl2A1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "2f7a5ca7-2674-4b57-9589-74f9ce595b24"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 4921), dtype=float32, numpy=\n",
              "array([[-0.07986051, -0.02821629, -0.0072703 , ..., -0.03879301,\n",
              "        -0.02286101, -0.04484978],\n",
              "       [-0.05277197, -0.00912803, -0.00510451, ..., -0.02388329,\n",
              "        -0.00487622, -0.03686088],\n",
              "       [-0.03598433, -0.01482207, -0.01312359, ..., -0.01404658,\n",
              "        -0.02321418, -0.03029034],\n",
              "       ...,\n",
              "       [-0.02261956, -0.01800052, -0.01717209, ..., -0.01200012,\n",
              "         0.01241633, -0.02250165],\n",
              "       [-0.05983879, -0.02388211, -0.00662596, ..., -0.03637584,\n",
              "        -0.0120089 , -0.06147018],\n",
              "       [-0.07153626, -0.02314482, -0.00849418, ..., -0.03579164,\n",
              "        -0.01180682, -0.05038257]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pqbg5o72A1l",
        "colab_type": "text"
      },
      "source": [
        "### 4) Define Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd9-JcTy2A1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    \n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    \n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hadERmRS2A1n",
        "colab_type": "text"
      },
      "source": [
        "### 5) Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXrfZW3E2A1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QMOs72_2A1s",
        "colab_type": "text"
      },
      "source": [
        "### 6）Train Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9qeriTL2A1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp,tar):\n",
        "    \n",
        "    loss = 0\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "    \n",
        "        encode_output,encode_hidden_state = encoder(inp)\n",
        "\n",
        "        decode_input = tf.convert_to_tensor([output_tokenizer.word_index['<start>']] * BATCH_SIZE)\n",
        "\n",
        "#         end_index = tf.cast(tf.argmin(tf.reduce_sum(targ,axis = 0)),tf.int32)\n",
        "\n",
        "        for t in range(1,tar.shape[1]):\n",
        "        \n",
        "            decode_input = targ[:,:t]\n",
        "        \n",
        "            predictions = decoder(decode_input,encode_output)\n",
        "\n",
        "            loss += loss_function(tar[:,t],predictions)\n",
        "        \n",
        "    \n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    \n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    \n",
        "    gradients = tape.gradient(loss,variables)\n",
        "    \n",
        "    \n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return batch_loss\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkmf929D2A1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sYDMlbhg2A13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c9db968-1c29-4226-9712-b375c014d7a3"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "    # 每 2 个周期（epoch），保存（检查点）一次模型\n",
        "    # if (epoch + 1) % 2 == 0:\n",
        "    #     checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "        \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.7033\n",
            "Epoch 1 Batch 100 Loss 0.6861\n",
            "Epoch 1 Batch 200 Loss 0.8124\n",
            "Epoch 1 Batch 300 Loss 0.6923\n",
            "Epoch 1 Loss 0.6809\n",
            "Time taken for 1 epoch 255.28366231918335 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.4936\n",
            "Epoch 2 Batch 100 Loss 0.6094\n",
            "Epoch 2 Batch 200 Loss 0.6386\n",
            "Epoch 2 Batch 300 Loss 0.4522\n",
            "Epoch 2 Loss 0.4715\n",
            "Time taken for 1 epoch 255.84842085838318 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.3371\n",
            "Epoch 3 Batch 100 Loss 0.3008\n",
            "Epoch 3 Batch 200 Loss 0.3419\n",
            "Epoch 3 Batch 300 Loss 0.2415\n",
            "Epoch 3 Loss 0.3591\n",
            "Time taken for 1 epoch 255.48558402061462 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2646\n",
            "Epoch 4 Batch 100 Loss 0.2403\n",
            "Epoch 4 Batch 200 Loss 0.2298\n",
            "Epoch 4 Batch 300 Loss 0.2870\n",
            "Epoch 4 Loss 0.2911\n",
            "Time taken for 1 epoch 256.9820203781128 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1636\n",
            "Epoch 5 Batch 100 Loss 0.2076\n",
            "Epoch 5 Batch 200 Loss 0.2050\n",
            "Epoch 5 Batch 300 Loss 0.2465\n",
            "Epoch 5 Loss 0.2384\n",
            "Time taken for 1 epoch 256.2942724227905 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1929\n",
            "Epoch 6 Batch 100 Loss 0.1961\n",
            "Epoch 6 Batch 200 Loss 0.2122\n",
            "Epoch 6 Batch 300 Loss 0.2272\n",
            "Epoch 6 Loss 0.2033\n",
            "Time taken for 1 epoch 256.4258587360382 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1593\n",
            "Epoch 7 Batch 100 Loss 0.1494\n",
            "Epoch 7 Batch 200 Loss 0.2049\n",
            "Epoch 7 Batch 300 Loss 0.2214\n",
            "Epoch 7 Loss 0.1770\n",
            "Time taken for 1 epoch 256.34021377563477 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1887\n",
            "Epoch 8 Batch 100 Loss 0.1491\n",
            "Epoch 8 Batch 200 Loss 0.1267\n",
            "Epoch 8 Batch 300 Loss 0.1534\n",
            "Epoch 8 Loss 0.1588\n",
            "Time taken for 1 epoch 257.1684491634369 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0787\n",
            "Epoch 9 Batch 100 Loss 0.1761\n",
            "Epoch 9 Batch 200 Loss 0.1145\n",
            "Epoch 9 Batch 300 Loss 0.2256\n",
            "Epoch 9 Loss 0.1452\n",
            "Time taken for 1 epoch 255.67920112609863 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0924\n",
            "Epoch 10 Batch 100 Loss 0.0805\n",
            "Epoch 10 Batch 200 Loss 0.1105\n",
            "Epoch 10 Batch 300 Loss 0.1535\n",
            "Epoch 10 Loss 0.1340\n",
            "Time taken for 1 epoch 255.77174472808838 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.1149\n",
            "Epoch 11 Batch 100 Loss 0.1381\n",
            "Epoch 11 Batch 200 Loss 0.1313\n",
            "Epoch 11 Batch 300 Loss 0.1025\n",
            "Epoch 11 Loss 0.1244\n",
            "Time taken for 1 epoch 256.19480895996094 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.1168\n",
            "Epoch 12 Batch 100 Loss 0.1495\n",
            "Epoch 12 Batch 200 Loss 0.1533\n",
            "Epoch 12 Batch 300 Loss 0.1199\n",
            "Epoch 12 Loss 0.1208\n",
            "Time taken for 1 epoch 256.1296651363373 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.1213\n",
            "Epoch 13 Batch 100 Loss 0.1013\n",
            "Epoch 13 Batch 200 Loss 0.1018\n",
            "Epoch 13 Batch 300 Loss 0.1446\n",
            "Epoch 13 Loss 0.1115\n",
            "Time taken for 1 epoch 256.2602450847626 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1264\n",
            "Epoch 14 Batch 100 Loss 0.0577\n",
            "Epoch 14 Batch 200 Loss 0.1319\n",
            "Epoch 14 Batch 300 Loss 0.1310\n",
            "Epoch 14 Loss 0.1045\n",
            "Time taken for 1 epoch 256.1768033504486 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0966\n",
            "Epoch 15 Batch 100 Loss 0.1030\n",
            "Epoch 15 Batch 200 Loss 0.0765\n",
            "Epoch 15 Batch 300 Loss 0.1384\n",
            "Epoch 15 Loss 0.1034\n",
            "Time taken for 1 epoch 256.9056513309479 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0499\n",
            "Epoch 16 Batch 100 Loss 0.0812\n",
            "Epoch 16 Batch 200 Loss 0.0903\n",
            "Epoch 16 Batch 300 Loss 0.0843\n",
            "Epoch 16 Loss 0.0969\n",
            "Time taken for 1 epoch 256.7323384284973 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0576\n",
            "Epoch 17 Batch 100 Loss 0.0554\n",
            "Epoch 17 Batch 200 Loss 0.0856\n",
            "Epoch 17 Batch 300 Loss 0.1032\n",
            "Epoch 17 Loss 0.0965\n",
            "Time taken for 1 epoch 256.3697717189789 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0482\n",
            "Epoch 18 Batch 100 Loss 0.0918\n",
            "Epoch 18 Batch 200 Loss 0.0576\n",
            "Epoch 18 Batch 300 Loss 0.0664\n",
            "Epoch 18 Loss 0.0931\n",
            "Time taken for 1 epoch 256.1762909889221 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0730\n",
            "Epoch 19 Batch 100 Loss 0.0721\n",
            "Epoch 19 Batch 200 Loss 0.1202\n",
            "Epoch 19 Batch 300 Loss 0.1253\n",
            "Epoch 19 Loss 0.0947\n",
            "Time taken for 1 epoch 256.83352756500244 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0926\n",
            "Epoch 20 Batch 100 Loss 0.0394\n",
            "Epoch 20 Batch 200 Loss 0.0459\n",
            "Epoch 20 Batch 300 Loss 0.0730\n",
            "Epoch 20 Loss 0.0878\n",
            "Time taken for 1 epoch 255.9155457019806 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfsfhaQL2A18",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b244b7eb-303c-47ad-b357-e460690ff973"
      },
      "source": [
        "# 恢复检查点目录 （checkpoint_dir） 中最新的检查点\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7faa4d54ca20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7cb8Dzc2A1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "#     attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [input_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_input,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "    \n",
        "#     encode_output,encode_hidden_state = encoder(inputs)\n",
        "    enc_out, enc_hidden = encoder(inputs)\n",
        "\n",
        "    dec_input = tf.expand_dims([output_tokenizer.word_index['<start>']],1)\n",
        "\n",
        "    predicted_ids = [1]\n",
        "    for t in range(max_length_targ):\n",
        "        \n",
        "        predictions = decoder(dec_input,enc_out)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        \n",
        "        if output_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        else:\n",
        "            if predicted_ids[-1]!=predicted_id:\n",
        "                predicted_ids.append(predicted_id)\n",
        "\n",
        "        result = ' '.join([output_tokenizer.index_word[predicted_id] for predicted_id in predicted_ids])\n",
        "\n",
        "        # 预测的 ID 被输送回模型\n",
        "        dec_input = tf.expand_dims(predicted_ids,0)\n",
        "        \n",
        "    \n",
        "    return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcyWk6xE2A2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW7LcFWi2A2E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8410b4d6-267c-4fa2-f0f9-2aee79bcf7ce"
      },
      "source": [
        "translate(u'no me gustan las.')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> no me gustan las . <end>\n",
            "Predicted translation: <start> i do not like them\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6AIQbn62A2I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "28a73738-0604-4808-97b4-b0b565d1d067"
      },
      "source": [
        "translate(u'¿todavia estan en casa.')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> ¿ todavia estan en casa . <end>\n",
            "Predicted translation: <start> are you still\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwsP0Nqa2A2K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e04dcede-8db2-4513-c138-946dbd1c88c2"
      },
      "source": [
        "translate(u'trata de averiguarlo.')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> trata de averiguarlo . <end>\n",
            "Predicted translation: <start> try\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPEznHlmMlhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03b6196b-9d92-4f47-fb9a-5e51280f3345"
      },
      "source": [
        "sp[8800][8:-6]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nos vemos a las cinco .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2oICU4xM8mS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "99126ffa-6982-4ea6-a905-dd13b5179719"
      },
      "source": [
        "translate(sp[8800][8:-6])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> nos vemos a las cinco . <end>\n",
            "Predicted translation: <start> see\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNdOIyxhNB0w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f81f2935-9ddf-4e8b-e232-fd287066bcf6"
      },
      "source": [
        "en[8800]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> see you at five . <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbPYN6iLNESP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
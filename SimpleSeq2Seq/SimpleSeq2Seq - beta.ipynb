{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tutorial of Simple Seq2seq with Teacher forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference : https://www.tensorflow.org/tutorials/text/nmt_with_attention?hl=zh_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Text PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （1） Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 unicode 文件转换为 ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    sentence = unicode_to_ascii(sentence.lower().strip())\n",
    "    \n",
    "    sentence = \" \".join([contractions[word] if word in contractions else word for word in sentence.split(' ') ][:-1])\n",
    "    \n",
    "    # 在单词与跟在其后的标点符号之间插入一个空格\n",
    "    # 例如： \"he is a boy.\" => \"he is a boy .\"\n",
    "    # 参考：https://stackoverflosentence.com/questions/3645931/python-padding-punctuation-sentenceith-sentencehite-spaces-keeping-punctuation\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # 除了 (a-z, A-Z, \".\", \"?\", \"!\", \",\")，将所有字符替换为空格\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.rstrip().strip()\n",
    "\n",
    "    # 给句子加上开始和结束标记\n",
    "    # 以便模型知道何时开始和结束预测\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 去除重音符号\n",
    "# 2. 清理句子\n",
    "# 3. 返回这样格式的单词对：[ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')[:-1]\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, sp = create_dataset(path_to_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i do not like <end>\n",
      "<start> no me gustan las <end>\n"
     ]
    }
   ],
   "source": [
    "print(en[20000])\n",
    "print(sp[20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) sentence tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    \n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    \n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    \n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "# def load_dataset(path, num_examples=None):\n",
    "#     # 创建清理过的输入输出对\n",
    "#     targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "#     input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "#     target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "#     return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # 创建清理过的输入输出对\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试实验不同大小的数据集\n",
    "num_examples = 30000\n",
    "input_tensor, output_tensor, input_tokenizer, output_tokenizer = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# 计算目标张量的最大长度 （max_length）\n",
    "max_length_targ, max_length_inp = max_length(output_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_output, max_length_input = max_length(output_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n"
     ]
    }
   ],
   "source": [
    "# 采用 80 - 20 的比例切分训练集和验证集\n",
    "input_tensor_train, input_tensor_val, output_tensor_train, output_tensor_val = train_test_split(input_tensor, output_tensor, test_size=0.2)\n",
    "\n",
    "# 显示长度\n",
    "print(len(input_tensor_train), len(output_tensor_train), len(input_tensor_val), len(output_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  87, 392,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "5 ----> tom\n",
      "15 ----> se\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "5 ----> tom\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(input_tokenizer, input_tensor_train[7000])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(output_tokenizer, output_tensor_train[7000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) DataSet Creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, output_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 14]), TensorShape([64, 9]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 14])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_input_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_dim,encode_units):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encode_units = encode_units\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size,self.embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(units=self.encode_units,return_sequences=True,return_state=True)\n",
    "        \n",
    "    \n",
    "    def call(self,x):\n",
    "        encoder_embedding = self.embedding(x)\n",
    "        encode_output,encode_hidden_state = self.gru(encoder_embedding)\n",
    "        \n",
    "        return encode_output,encode_hidden_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_VOCAB_SIZE = len(input_tokenizer.word_index)+1\n",
    "ENCODER_EMBEDDING_SIZE = 256\n",
    "ENCODER_UNIT = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4916"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_VOCAB_SIZE,ENCODER_EMBEDDING_SIZE,ENCODER_UNIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_output,encode_hidden_state = encoder(example_input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=159, shape=(64, 128), dtype=float32, numpy=\n",
       "array([[ 0.06025203,  0.06018748,  0.00585309, ...,  0.03294687,\n",
       "        -0.04083669,  0.0157035 ],\n",
       "       [ 0.06031514,  0.06010835,  0.00571282, ...,  0.03294995,\n",
       "        -0.0408518 ,  0.01564984],\n",
       "       [ 0.06051484,  0.06023348,  0.00582696, ...,  0.03311078,\n",
       "        -0.04075034,  0.01580523],\n",
       "       ...,\n",
       "       [ 0.06039887,  0.06022925,  0.00583461, ...,  0.03299372,\n",
       "        -0.04072356,  0.01577699],\n",
       "       [ 0.05995977,  0.0598466 ,  0.005825  , ...,  0.03229364,\n",
       "        -0.04065358,  0.01509264],\n",
       "       [ 0.06022097,  0.05999106,  0.00582924, ...,  0.03280593,\n",
       "        -0.04075763,  0.01562667]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,vocab_size,decode_unit,embedding_dim):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.decode_unit = decode_unit\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        ### structure\n",
    "        self.gru =  tf.keras.layers.GRU(units=self.decode_unit,return_sequences=True,return_state=True)\n",
    "        \n",
    "        self.embeddding = tf.keras.layers.Embedding(self.vocab_size,self.embedding_dim)\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(self.vocab_size)\n",
    "        \n",
    "        \n",
    "        ### decode_input  【batch_size,word_index】\n",
    "    def call(self,decode_input,encode_output):\n",
    "        \n",
    "        decode_input = self.embeddding(decode_input)\n",
    "        \n",
    "        shape = (decode_input.shape[0],encode_output.shape[1]-decode_input.shape[1],decode_input.shape[2])\n",
    "        \n",
    "        padding = tf.zeros(shape)\n",
    "        \n",
    "        decode_input = tf.concat([decode_input,padding],axis = 1)\n",
    "        \n",
    "        concat_vector = tf.concat([encode_output,decode_input,decode_input],axis = -1)\n",
    "        \n",
    "        decode_output,decode_hidden_state = self.gru(concat_vector)\n",
    "        \n",
    "        decode_output = tf.reduce_sum(decode_output,axis = 1)\n",
    "        \n",
    "        y = self.fc(decode_output)\n",
    "        \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameter\n",
    "output_vocab_size = len(output_tokenizer.word_index)+1\n",
    "\n",
    "DECODER_UNIT = 256\n",
    "\n",
    "encode_embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2642"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(output_vocab_size,DECODER_UNIT,encode_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_input = example_target_batch[:, :1]\n",
    "# decode_input = tf.convert_to_tensor([output_tokenizer.word_index['<start>']] * BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = decoder(decode_input,encode_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=549, shape=(64, 2642), dtype=float32, numpy=\n",
       "array([[ 0.00443673, -0.01104843,  0.00246613, ..., -0.02069634,\n",
       "        -0.04083196,  0.03339528],\n",
       "       [ 0.00073739, -0.00400375,  0.01410341, ..., -0.01000342,\n",
       "        -0.03755658,  0.04245183],\n",
       "       [-0.0041207 , -0.01033499,  0.01618817, ..., -0.02738772,\n",
       "        -0.04122337,  0.04896358],\n",
       "       ...,\n",
       "       [-0.02188004, -0.01317842,  0.00931021, ..., -0.02218042,\n",
       "        -0.04914907,  0.03876209],\n",
       "       [-0.01812549, -0.0052731 ,  0.00459114, ..., -0.02370732,\n",
       "        -0.02571396,  0.03012667],\n",
       "       [-0.00816171, -0.01248191,  0.00289739, ..., -0.01406125,\n",
       "        -0.04600342,  0.0416571 ]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    \n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6）Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp,tar):\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "    \n",
    "        encode_output,encode_hidden_state = encoder(inp)\n",
    "\n",
    "        decode_input = tf.convert_to_tensor([output_tokenizer.word_index['<start>']] * BATCH_SIZE)\n",
    "\n",
    "#         end_index = tf.cast(tf.argmin(tf.reduce_sum(targ,axis = 0)),tf.int32)\n",
    "\n",
    "        for t in range(1,tar.shape[1]):\n",
    "        \n",
    "            decode_input = targ[:,:t]\n",
    "        \n",
    "            predictions = decoder(decode_input,encode_output)\n",
    "\n",
    "            loss += loss_function(tar[:,t],predictions)\n",
    "        \n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    \n",
    "    gradients = tape.gradient(loss,variables)\n",
    "    \n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.5259\n",
      "Epoch 1 Batch 100 Loss 0.5203\n",
      "Epoch 1 Batch 200 Loss 0.3780\n",
      "Epoch 1 Batch 300 Loss 0.3685\n",
      "Epoch 2 Batch 0 Loss 0.4723\n",
      "Epoch 2 Batch 100 Loss 0.4364\n",
      "Epoch 2 Batch 200 Loss 0.3229\n",
      "Epoch 2 Batch 300 Loss 0.3291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-3'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 0.3300\n",
      "Time taken for 1 epoch 92.37497138977051 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.4399\n",
      "Epoch 3 Batch 100 Loss 0.3781\n",
      "Epoch 3 Batch 200 Loss 0.2927\n",
      "Epoch 3 Batch 300 Loss 0.3039\n",
      "Epoch 4 Batch 0 Loss 0.3827\n",
      "Epoch 4 Batch 100 Loss 0.3308\n",
      "Epoch 4 Batch 200 Loss 0.2431\n",
      "Epoch 4 Batch 300 Loss 0.2933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-4'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss 0.2632\n",
      "Time taken for 1 epoch 93.03660202026367 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.3563\n",
      "Epoch 5 Batch 100 Loss 0.2892\n",
      "Epoch 5 Batch 200 Loss 0.2124\n",
      "Epoch 5 Batch 300 Loss 0.2717\n",
      "Epoch 6 Batch 0 Loss 0.3354\n",
      "Epoch 6 Batch 100 Loss 0.2745\n",
      "Epoch 6 Batch 200 Loss 0.1752\n",
      "Epoch 6 Batch 300 Loss 0.2379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-5'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 0.2128\n",
      "Time taken for 1 epoch 92.72807574272156 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.3061\n",
      "Epoch 7 Batch 100 Loss 0.2211\n",
      "Epoch 7 Batch 200 Loss 0.1513\n",
      "Epoch 7 Batch 300 Loss 0.2107\n",
      "Epoch 8 Batch 0 Loss 0.2896\n",
      "Epoch 8 Batch 100 Loss 0.2053\n",
      "Epoch 8 Batch 200 Loss 0.1417\n",
      "Epoch 8 Batch 300 Loss 0.2010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-6'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss 0.1776\n",
      "Time taken for 1 epoch 92.68789458274841 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.2864\n",
      "Epoch 9 Batch 100 Loss 0.1870\n",
      "Epoch 9 Batch 200 Loss 0.1535\n",
      "Epoch 9 Batch 300 Loss 0.1845\n",
      "Epoch 10 Batch 0 Loss 0.2716\n",
      "Epoch 10 Batch 100 Loss 0.1167\n",
      "Epoch 10 Batch 200 Loss 0.1219\n",
      "Epoch 10 Batch 300 Loss 0.1853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-7'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss 0.1530\n",
      "Time taken for 1 epoch 92.4054696559906 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.2276\n",
      "Epoch 11 Batch 100 Loss 0.1447\n",
      "Epoch 11 Batch 200 Loss 0.0976\n",
      "Epoch 11 Batch 300 Loss 0.1777\n",
      "Epoch 12 Batch 0 Loss 0.2141\n",
      "Epoch 12 Batch 100 Loss 0.1348\n",
      "Epoch 12 Batch 200 Loss 0.0893\n",
      "Epoch 12 Batch 300 Loss 0.1648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-8'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss 0.1347\n",
      "Time taken for 1 epoch 91.73920321464539 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.2191\n",
      "Epoch 13 Batch 100 Loss 0.1280\n",
      "Epoch 13 Batch 200 Loss 0.0888\n",
      "Epoch 13 Batch 300 Loss 0.1569\n",
      "Epoch 14 Batch 0 Loss 0.1961\n",
      "Epoch 14 Batch 100 Loss 0.1092\n",
      "Epoch 14 Batch 200 Loss 0.0874\n",
      "Epoch 14 Batch 300 Loss 0.1511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-9'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss 0.1223\n",
      "Time taken for 1 epoch 85.86132621765137 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.2275\n",
      "Epoch 15 Batch 100 Loss 0.1175\n",
      "Epoch 15 Batch 200 Loss 0.0758\n",
      "Epoch 15 Batch 300 Loss 0.1489\n",
      "Epoch 16 Batch 0 Loss 0.2160\n",
      "Epoch 16 Batch 100 Loss 0.1013\n",
      "Epoch 16 Batch 200 Loss 0.0716\n",
      "Epoch 16 Batch 300 Loss 0.1370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-10'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss 0.1052\n",
      "Time taken for 1 epoch 91.47631859779358 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.2040\n",
      "Epoch 17 Batch 100 Loss 0.0829\n",
      "Epoch 17 Batch 200 Loss 0.0810\n",
      "Epoch 17 Batch 300 Loss 0.1248\n",
      "Epoch 18 Batch 0 Loss 0.1732\n",
      "Epoch 18 Batch 100 Loss 0.0771\n",
      "Epoch 18 Batch 200 Loss 0.0823\n",
      "Epoch 18 Batch 300 Loss 0.1250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-11'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss 0.0982\n",
      "Time taken for 1 epoch 84.03984785079956 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.1877\n",
      "Epoch 19 Batch 100 Loss 0.0893\n",
      "Epoch 19 Batch 200 Loss 0.0849\n",
      "Epoch 19 Batch 300 Loss 0.1367\n",
      "Epoch 20 Batch 0 Loss 0.2016\n",
      "Epoch 20 Batch 100 Loss 0.0954\n",
      "Epoch 20 Batch 200 Loss 0.0876\n",
      "Epoch 20 Batch 300 Loss 0.1161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt-12'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss 0.0932\n",
      "Time taken for 1 epoch 91.63279891014099 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "    # 每 2 个周期（epoch），保存（检查点）一次模型\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1,total_loss / steps_per_epoch))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x21912183c88>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 恢复检查点目录 （checkpoint_dir） 中最新的检查点\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "#     attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [input_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_input,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    \n",
    "#     encode_output,encode_hidden_state = encoder(inputs)\n",
    "    enc_out, enc_hidden = encoder(inputs)\n",
    "\n",
    "    dec_input = tf.expand_dims([output_tokenizer.word_index['<start>']],1)\n",
    "\n",
    "    predicted_ids = [1]\n",
    "    for t in range(max_length_targ):\n",
    "        \n",
    "        predictions = decoder(dec_input,enc_out)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        \n",
    "        if output_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        else:\n",
    "            if  predicted_ids[-1]!=predicted_id:\n",
    "                predicted_ids.append(predicted_id)\n",
    "\n",
    "        result = ' '.join([output_tokenizer.index_word[predicted_id] for predicted_id in predicted_ids])\n",
    "\n",
    "        # 预测的 ID 被输送回模型\n",
    "        dec_input = tf.expand_dims(predicted_ids,0)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> no me gustan <end>\n",
      "Predicted translation: <start> i do not like\n"
     ]
    }
   ],
   "source": [
    "translate(u'no me gustan las')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ¿ todavia estan en <end>\n",
      "Predicted translation: <start> are you still\n"
     ]
    }
   ],
   "source": [
    "translate(u'¿todavia estan en casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> trata de <end>\n",
      "Predicted translation: <start> try\n"
     ]
    }
   ],
   "source": [
    "translate(u'trata de averiguarlo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Summarizing Text with Amazon Reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yMzSA0gJJlHt",
        "xoFD7gx5JlIK",
        "nCCeVxqCJlI4",
        "sBGAptnqJlJH",
        "F4RuDac3JlJV",
        "uELVR9fvJlJg",
        "JpLxGrqlJlJ5",
        "ILINEyLKJlKF"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LX_Kcz8JlF4",
        "colab_type": "text"
      },
      "source": [
        "# Summarizing Text with Amazon Reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLoBjvUpJlF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import io\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "import unicodedata\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5aE35HfJlF9",
        "colab_type": "text"
      },
      "source": [
        "## 1）Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vaiku88AOATE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "6d5fec01-0253-4fd0-ec79-596dd5a0bfe3"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmVSQ-6PN66t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "reviews = pd.read_csv(\"drive/My Drive/Colab Notebooks/data/Reviews.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7tcbXMrJlGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "74e3560d-f7f0-41c2-dbb4-6d3b44da50b0"
      },
      "source": [
        "reviews[reviews.index == 835]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>836</td>\n",
              "      <td>B001ELL9X6</td>\n",
              "      <td>A38OVPNVBVUO9Y</td>\n",
              "      <td>T. Burton \"Hakalau Tom\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1349222400</td>\n",
              "      <td>great organic tea at a great price</td>\n",
              "      <td>&lt;a href=\"http://www.amazon.com/gp/product/B001...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Id  ...                                               Text\n",
              "835  836  ...  <a href=\"http://www.amazon.com/gp/product/B001...\n",
              "\n",
              "[1 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrvEE6ckJlGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = reviews.head(5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCuZWWrqJlGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove null values and unneeded features\n",
        "reviews = reviews.dropna()\n",
        "reviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator',\n",
        "                        'Score','Time'], 1)\n",
        "reviews = reviews.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6vucfvVJlGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "04ac678a-5a08-4de5-db8c-f2f50f13ce0d"
      },
      "source": [
        "reviews.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Summary                                               Text\n",
              "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
              "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
              "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
              "4            Great taffy  Great taffy at a great price.  There was a wid..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcpxbd8hJlGd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "92d57866-e596-447e-b328-e05689755d4d"
      },
      "source": [
        "reviews.columns"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Summary', 'Text'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UY6UZ7JJlGj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "30b68645-31a7-41e7-ef58-75233d6d5a79"
      },
      "source": [
        "# Inspecting some of the reviews\n",
        "for i in range(5):\n",
        "    print(\"Review #\",i+1)\n",
        "    print(reviews.Summary[i])\n",
        "    print(reviews.Text[i])\n",
        "    print()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review # 1\n",
            "Good Quality Dog Food\n",
            "I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
            "\n",
            "Review # 2\n",
            "Not as Advertised\n",
            "Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
            "\n",
            "Review # 3\n",
            "\"Delight\" says it all\n",
            "This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
            "\n",
            "Review # 4\n",
            "Cough Medicine\n",
            "If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
            "\n",
            "Review # 5\n",
            "Great taffy\n",
            "Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd2DxxrDJlGl",
        "colab_type": "text"
      },
      "source": [
        "## 2) Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l93uwLERJlGm",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk6KFDa-JlGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RHxaRegJlGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c660d21e-82c6-4261-8843-339d84cbb62c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMqGAD3MJlGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
        "# # (https://github.com/commonsense/conceptnet-numberbatch)\n",
        "# embeddings_index = {}\n",
        "# with open('./data/numberbatch-en.txt', encoding='utf-8') as f:\n",
        "#     for line in f:\n",
        "#         values = line.split(' ')\n",
        "#         word = values[0]\n",
        "#         embedding = np.asarray(values[1:], dtype='float32')\n",
        "#         embeddings_index[word] = embedding\n",
        "\n",
        "# print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo8-bEK1JlG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = 150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRjyWEoGJlG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(sentence,remove_stopwords):\n",
        "    \n",
        "    sentence = \" \".join([contractions[word] if word in contractions else word for word in sentence.lower().split(' ') ])\n",
        "    \n",
        "    sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sentence, flags=re.MULTILINE)\n",
        "    sentence = re.sub(r'\\<a href', ' ', sentence)\n",
        "    sentence = re.sub(r'&amp;', '', sentence) \n",
        "    sentence = re.sub(r'[_\"~\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', sentence)\n",
        "    sentence = re.sub(r'<br />', ' ', sentence)\n",
        "    sentence = re.sub(r'<br>', ' ', sentence)\n",
        "    sentence = re.sub(r'>', ' ', sentence)\n",
        "    sentence = re.sub(r'<', ' ', sentence)\n",
        "    sentence = re.sub(r'\\'', ' ', sentence)\n",
        "    \n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        sentence = sentence.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        sentence = [w for w in sentence if not w in stops]\n",
        "        sentence = \" \".join(sentence)\n",
        "        \n",
        "    sentence = \" \".join(sentence.split()[:max_length])\n",
        "    \n",
        "#     sentence += ' .'\n",
        "    \n",
        "#     sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkaRlgLfJlG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = \"If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmDsdjtPJlHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6ad6749-e43c-40ee-e4d0-eabdd1074aea"
      },
      "source": [
        "preprocess_sentence(sentence,True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lmfu1AkJlHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews['Text'] = reviews.apply(lambda row: preprocess_sentence(row['Text'],True),axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp1irH5PJlHH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "eef6fe5f-d5b9-4edf-f0e5-757c7d6b8956"
      },
      "source": [
        "reviews.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>bought several vitality canned dog food produc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>confection around centuries light pillowy citr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>looking secret ingredient robitussin believe f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy</td>\n",
              "      <td>great taffy great price wide assortment yummy ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Summary                                               Text\n",
              "0  Good Quality Dog Food  bought several vitality canned dog food produc...\n",
              "1      Not as Advertised  product arrived labeled jumbo salted peanuts p...\n",
              "2  \"Delight\" says it all  confection around centuries light pillowy citr...\n",
              "3         Cough Medicine  looking secret ingredient robitussin believe f...\n",
              "4            Great taffy  great taffy great price wide assortment yummy ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlZBPmr2JlHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews['Summary'] = reviews.apply(lambda row: preprocess_sentence(row['Summary'],False),axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VY4STUwJlHQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "df53dda2-bef7-4082-fc7c-92f185f01c77"
      },
      "source": [
        "reviews.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>good quality dog food</td>\n",
              "      <td>bought several vitality canned dog food produc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>not as advertised</td>\n",
              "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>delight says it all</td>\n",
              "      <td>confection around centuries light pillowy citr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cough medicine</td>\n",
              "      <td>looking secret ingredient robitussin believe f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>great taffy</td>\n",
              "      <td>great taffy great price wide assortment yummy ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Summary                                               Text\n",
              "0  good quality dog food  bought several vitality canned dog food produc...\n",
              "1      not as advertised  product arrived labeled jumbo salted peanuts p...\n",
              "2    delight says it all  confection around centuries light pillowy citr...\n",
              "3         cough medicine  looking secret ingredient robitussin believe f...\n",
              "4            great taffy  great taffy great price wide assortment yummy ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCnfnqrkJlHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews['text_length'] = [len(text.split()) for text in reviews['Text']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVfS4zsmJlHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "8afce4c5-de45-4e6e-e6c6-cec56809a9da"
      },
      "source": [
        "reviews.sort_values(by='text_length')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>text_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4466</th>\n",
              "      <td>woderful wild rasberry french twists</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>713</th>\n",
              "      <td>these chips are addictive</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1345</th>\n",
              "      <td>wish bone fat free ranch dressing</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2032</th>\n",
              "      <td>family breakfast grand kids kids 20 of us</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2304</th>\n",
              "      <td>great low calorie treat for overweight dogs or...</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2943</th>\n",
              "      <td>kitty junk food</td>\n",
              "      <td>five cats one elderly cat 15 years four 9 11 y...</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2946</th>\n",
              "      <td>kitty junk food</td>\n",
              "      <td>five cats one elderly cat 15 years four 9 11 y...</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>warning warning alcohol sugars</td>\n",
              "      <td>buyer beware please sweetener everybody maltit...</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4552</th>\n",
              "      <td>one of the best decaffeninated coffees i have ...</td>\n",
              "      <td>coffee fan drinks coffee black nothing added a...</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1164</th>\n",
              "      <td>not nearly as good as bissinger s french long</td>\n",
              "      <td>searching constant less expensive supply real ...</td>\n",
              "      <td>150</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Summary  ... text_length\n",
              "4466               woderful wild rasberry french twists  ...           0\n",
              "713                           these chips are addictive  ...           0\n",
              "1345                  wish bone fat free ranch dressing  ...           0\n",
              "2032          family breakfast grand kids kids 20 of us  ...           0\n",
              "2304  great low calorie treat for overweight dogs or...  ...           0\n",
              "...                                                 ...  ...         ...\n",
              "2943                                    kitty junk food  ...         150\n",
              "2946                                    kitty junk food  ...         150\n",
              "73                       warning warning alcohol sugars  ...         150\n",
              "4552  one of the best decaffeninated coffees i have ...  ...         150\n",
              "1164      not nearly as good as bissinger s french long  ...         150\n",
              "\n",
              "[5000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUSzkzn2JlHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_token(sentence):\n",
        "    sentence += ' .'\n",
        "    \n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qx5ugG7JlHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews['Summary'] = reviews.apply(lambda row: add_token(row['Summary']),axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1YKNgk-JlHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews['Text'] = reviews.apply(lambda row: add_token(row['Text']),axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5eGbKtzJlHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews.index = range(len(reviews))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMtQ_LXIJlHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "f701f8f7-0332-4d10-9bb8-c8ae82b1d450"
      },
      "source": [
        "# Inspecting some of the reviews\n",
        "for i in range(5):\n",
        "    print(\"Review #\",i+1)\n",
        "    print(reviews.Summary[i])\n",
        "    print(reviews.Text[i])\n",
        "    print()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review # 1\n",
            "<start> good quality dog food . <end>\n",
            "<start> bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better . <end>\n",
            "\n",
            "Review # 2\n",
            "<start> not as advertised . <end>\n",
            "<start> product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo . <end>\n",
            "\n",
            "Review # 3\n",
            "<start> delight says it all . <end>\n",
            "<start> confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story c lewis lion witch wardrobe treat seduces edmund selling brother sisters witch . <end>\n",
            "\n",
            "Review # 4\n",
            "<start> cough medicine . <end>\n",
            "<start> looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal . <end>\n",
            "\n",
            "Review # 5\n",
            "<start> great taffy . <end>\n",
            "<start> great taffy great price wide assortment yummy taffy delivery quick taffy lover deal . <end>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMzSA0gJJlHt",
        "colab_type": "text"
      },
      "source": [
        "#### 1) Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgIV3kVPJlHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    \n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    \n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "    \n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfq94O1TJlHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset():\n",
        "\n",
        "    targ_lang, inp_lang = reviews['Summary'].values,reviews['Text'].values\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCy3_SFEJlH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, output_tensor, input_tokenizer, output_tokenizer = load_dataset()\n",
        "\n",
        "max_length_targ, max_length_inp = get_max_length(output_tensor), get_max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pn4L-UPJlH4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "babae305-d138-42bd-f4c2-714e54b99113"
      },
      "source": [
        "max_length_targ, max_length_inp"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33, 153)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpN1KufKJlH-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "47d16529-94e5-42fe-9ff2-ad485a3e4c8f"
      },
      "source": [
        "# 采用 80 - 20 的比例切分训练集和验证集\n",
        "input_tensor_train, input_tensor_val, output_tensor_train, output_tensor_val = train_test_split(input_tensor, output_tensor, test_size=0.2)\n",
        "\n",
        "# 显示长度\n",
        "print(len(input_tensor_train), len(output_tensor_train), len(input_tensor_val), len(output_tensor_val))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000 4000 1000 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MUCrcpwJlIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEUThAHkJlIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "90995252-ecfc-4f36-b97b-71e0b4cd3e15"
      },
      "source": [
        "input_tokenizer.index_word[4]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'br'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "QYE79u3VJlIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "17ef0ead-60dc-45e2-f8f5-139d916717e0"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(input_tokenizer, input_tensor_train[30])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(output_tokenizer, output_tensor_train[30])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "5335 ----> koeze\n",
            "11386 ----> colossal\n",
            "2470 ----> cashews\n",
            "60 ----> delicious\n",
            "2470 ----> cashews\n",
            "86 ----> ever\n",
            "166 ----> tasted\n",
            "655 ----> huge\n",
            "4 ----> br\n",
            "2195 ----> kidding\n",
            "896 ----> call\n",
            "4417 ----> jumbo\n",
            "421 ----> flavorful\n",
            "4 ----> br\n",
            "58 ----> order\n",
            "256 ----> disappointed\n",
            "4 ----> br\n",
            "577 ----> expect\n",
            "276 ----> wish\n",
            "94 ----> ordered\n",
            "443 ----> larger\n",
            "127 ----> size\n",
            "2 ----> .\n",
            "3 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "9 ----> best\n",
            "2775 ----> cashews\n",
            "2 ----> .\n",
            "3 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoFD7gx5JlIK",
        "colab_type": "text"
      },
      "source": [
        "#### 3) DataSet Creating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbB7wQwmJlIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7192f650-7bad-407c-e82e-1977301733fa"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, output_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 153]), TensorShape([64, 33]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BdkRmqGJlIO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8c7a7e84-94c6-494b-b129-265168adcb50"
      },
      "source": [
        "example_input_batch"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 153), dtype=int32, numpy=\n",
              "array([[   1,   10,  275, ...,    0,    0,    0],\n",
              "       [   1,  131,   49, ...,    0,    0,    0],\n",
              "       [   1,    7,   10, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [   1,  150, 7610, ...,    0,    0,    0],\n",
              "       [   1,   24, 1131, ...,    0,    0,    0],\n",
              "       [   1,  315,   39, ...,    0,    0,    0]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuUO3b4VJlIV",
        "colab_type": "text"
      },
      "source": [
        "## 3) Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2fdmamAJlIW",
        "colab_type": "text"
      },
      "source": [
        "##### (1 )Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCPkbbWdJlIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GVnDob2JlIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "    # 将 sin 应用于数组中的偶数索引（indices）；2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # 将 cos 应用于数组中的奇数索引；2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCCeVxqCJlI4",
        "colab_type": "text"
      },
      "source": [
        "#### (2) Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRJFeb8wJlI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # 添加额外的维度来将填充加到\n",
        "    # 注意力对数（logits）。\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQEX3DLdJlJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBGAptnqJlJH",
        "colab_type": "text"
      },
      "source": [
        "#### (3) Multi-head Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzIg3rMDJlJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"计算注意力权重。\n",
        "    q, k, v 必须具有匹配的前置维度。\n",
        "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。\n",
        "    虽然 mask 根据其类型（填充或前瞻）有不同的形状，\n",
        "    但是 mask 必须能进行广播转换以便求和。\n",
        "\n",
        "    参数:\n",
        "    q: 请求的形状 == (..., seq_len_q, depth)\n",
        "    k: 主键的形状 == (..., seq_len_k, depth)\n",
        "    v: 数值的形状 == (..., seq_len_v, depth_v)\n",
        "    mask: Float 张量，其形状能转换成\n",
        "          (..., seq_len_q, seq_len_k)。默认为None。\n",
        "\n",
        "    返回值:\n",
        "    输出，注意力权重\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # 缩放 matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # 将 mask 加入到缩放的张量上。\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    # softmax 在最后一个轴（seq_len_k）上归一化，因此分数\n",
        "    # 相加等于1。\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFZDEr-HJlJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"分拆最后一个维度到 (num_heads, depth).\n",
        "        转置结果使得形状为 (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4RuDac3JlJV",
        "colab_type": "text"
      },
      "source": [
        "#### (4) Point wise feed forward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o2UdQ5FJlJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uELVR9fvJlJg",
        "colab_type": "text"
      },
      "source": [
        "#### (5) Encoder Layer and Decoder Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5L_Gox8JlJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxnpntN-JlJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, x, enc_output, training,look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGruF2AdJlJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # 将嵌入和位置编码相加。\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hczU4YX7JlJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training,look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpLxGrqlJlJ5",
        "colab_type": "text"
      },
      "source": [
        "#### (5) Transfomer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOF5HoszJlJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask,look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DETXP5jyJlJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "dc9491a0-7375-4f6c-e1f2-340137e7912e"
      },
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
        "    input_vocab_size=8500, target_vocab_size=8000, \n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 62))\n",
        "temp_target = tf.random.uniform((64, 26))\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
        "                               enc_padding_mask=None, \n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 26, 8000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILINEyLKJlKF",
        "colab_type": "text"
      },
      "source": [
        "#### (5) hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtX3nGMxJlKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = len(input_tokenizer.word_index)+2\n",
        "target_vocab_size = len(output_tokenizer.word_index) + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQej0Gb3JlKL",
        "colab_type": "text"
      },
      "source": [
        "### (4) Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6UYsRytJlKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCoD9oa9JlKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L8Rsgp5JlKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "e5def7b5-c4ae-4592-c663-00e15f0640f0"
      },
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lbpdWcTJlKg",
        "colab_type": "text"
      },
      "source": [
        "### (5) Loss and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Q6hNliJlKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m-y_itgJlKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9biFojwJJlKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuGhgjisJlKx",
        "colab_type": "text"
      },
      "source": [
        "### (6) Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duTssH6mJlKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwln6BjXJlK4",
        "colab_type": "text"
      },
      "source": [
        "### (7) Train Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EQR4Q8iJlK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    # 编码器填充遮挡\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # 在解码器的第二个注意力模块使用。\n",
        "    # 该填充遮挡用于遮挡编码器的输出。\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # 在解码器的第一个注意力模块使用。\n",
        "    # 用于填充（pad）和遮挡（mask）解码器获取到的输入的后续标记（future tokens）。\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiYgo72NJlK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# 如果检查点存在，则恢复最新的检查点。\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UjROmAxJlLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GyWHvOpJlLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                     True, \n",
        "                                     enc_padding_mask, \n",
        "                                     combined_mask, \n",
        "                                     dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l93TArOuJlLN",
        "colab_type": "text"
      },
      "source": [
        "### (8) Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXLZOxkpJlLP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7967d4a-5c0e-4d07-ac58-8c4e3399eb4a"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                             ckpt_save_path))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.5309 Accuracy 0.0908\n",
            "Epoch 1 Batch 50 Loss 0.5577 Accuracy 0.0931\n",
            "Epoch 1 Loss 0.5625 Accuracy 0.0932\n",
            "Time taken for 1 epoch: 14.660086154937744 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.5468 Accuracy 0.0913\n",
            "Epoch 2 Batch 50 Loss 0.5294 Accuracy 0.0964\n",
            "Epoch 2 Loss 0.5350 Accuracy 0.0963\n",
            "Time taken for 1 epoch: 14.5689377784729 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4077 Accuracy 0.0962\n",
            "Epoch 3 Batch 50 Loss 0.5072 Accuracy 0.0999\n",
            "Epoch 3 Loss 0.5085 Accuracy 0.0996\n",
            "Time taken for 1 epoch: 14.533266544342041 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.5029 Accuracy 0.0991\n",
            "Epoch 4 Batch 50 Loss 0.4805 Accuracy 0.1018\n",
            "Epoch 4 Loss 0.4822 Accuracy 0.1023\n",
            "Time taken for 1 epoch: 14.669723510742188 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4372 Accuracy 0.1035\n",
            "Epoch 5 Batch 50 Loss 0.4543 Accuracy 0.1060\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-5\n",
            "Epoch 5 Loss 0.4557 Accuracy 0.1061\n",
            "Time taken for 1 epoch: 15.021337985992432 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.4215 Accuracy 0.1069\n",
            "Epoch 6 Batch 50 Loss 0.4306 Accuracy 0.1087\n",
            "Epoch 6 Loss 0.4303 Accuracy 0.1089\n",
            "Time taken for 1 epoch: 14.680190324783325 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.3617 Accuracy 0.1147\n",
            "Epoch 7 Batch 50 Loss 0.4033 Accuracy 0.1124\n",
            "Epoch 7 Loss 0.4058 Accuracy 0.1121\n",
            "Time taken for 1 epoch: 14.558656930923462 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.4260 Accuracy 0.1143\n",
            "Epoch 8 Batch 50 Loss 0.3774 Accuracy 0.1165\n",
            "Epoch 8 Loss 0.3766 Accuracy 0.1165\n",
            "Time taken for 1 epoch: 14.744022846221924 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.3379 Accuracy 0.1167\n",
            "Epoch 9 Batch 50 Loss 0.3491 Accuracy 0.1200\n",
            "Epoch 9 Loss 0.3516 Accuracy 0.1197\n",
            "Time taken for 1 epoch: 14.650104999542236 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3084 Accuracy 0.1201\n",
            "Epoch 10 Batch 50 Loss 0.3302 Accuracy 0.1231\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-6\n",
            "Epoch 10 Loss 0.3296 Accuracy 0.1232\n",
            "Time taken for 1 epoch: 15.101315975189209 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.3030 Accuracy 0.1304\n",
            "Epoch 11 Batch 50 Loss 0.2983 Accuracy 0.1275\n",
            "Epoch 11 Loss 0.3013 Accuracy 0.1274\n",
            "Time taken for 1 epoch: 14.645387887954712 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.2386 Accuracy 0.1338\n",
            "Epoch 12 Batch 50 Loss 0.2781 Accuracy 0.1317\n",
            "Epoch 12 Loss 0.2796 Accuracy 0.1312\n",
            "Time taken for 1 epoch: 14.70119047164917 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2855 Accuracy 0.1387\n",
            "Epoch 13 Batch 50 Loss 0.2568 Accuracy 0.1368\n",
            "Epoch 13 Loss 0.2584 Accuracy 0.1359\n",
            "Time taken for 1 epoch: 14.815110921859741 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.2133 Accuracy 0.1416\n",
            "Epoch 14 Batch 50 Loss 0.2336 Accuracy 0.1415\n",
            "Epoch 14 Loss 0.2336 Accuracy 0.1403\n",
            "Time taken for 1 epoch: 14.660710096359253 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.2034 Accuracy 0.1523\n",
            "Epoch 15 Batch 50 Loss 0.2108 Accuracy 0.1452\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-7\n",
            "Epoch 15 Loss 0.2129 Accuracy 0.1446\n",
            "Time taken for 1 epoch: 15.005358219146729 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1547 Accuracy 0.1411\n",
            "Epoch 16 Batch 50 Loss 0.1910 Accuracy 0.1485\n",
            "Epoch 16 Loss 0.1936 Accuracy 0.1486\n",
            "Time taken for 1 epoch: 14.66184949874878 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.1936 Accuracy 0.1602\n",
            "Epoch 17 Batch 50 Loss 0.1712 Accuracy 0.1528\n",
            "Epoch 17 Loss 0.1731 Accuracy 0.1529\n",
            "Time taken for 1 epoch: 14.748253583908081 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.1351 Accuracy 0.1670\n",
            "Epoch 18 Batch 50 Loss 0.1547 Accuracy 0.1574\n",
            "Epoch 18 Loss 0.1542 Accuracy 0.1567\n",
            "Time taken for 1 epoch: 14.749034404754639 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1227 Accuracy 0.1602\n",
            "Epoch 19 Batch 50 Loss 0.1404 Accuracy 0.1604\n",
            "Epoch 19 Loss 0.1401 Accuracy 0.1600\n",
            "Time taken for 1 epoch: 14.82104206085205 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.1089 Accuracy 0.1572\n",
            "Epoch 20 Batch 50 Loss 0.1228 Accuracy 0.1646\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-8\n",
            "Epoch 20 Loss 0.1232 Accuracy 0.1637\n",
            "Time taken for 1 epoch: 14.991357326507568 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0965 Accuracy 0.1758\n",
            "Epoch 21 Batch 50 Loss 0.1144 Accuracy 0.1666\n",
            "Epoch 21 Loss 0.1137 Accuracy 0.1654\n",
            "Time taken for 1 epoch: 14.681220531463623 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0959 Accuracy 0.1812\n",
            "Epoch 22 Batch 50 Loss 0.0968 Accuracy 0.1684\n",
            "Epoch 22 Loss 0.0993 Accuracy 0.1686\n",
            "Time taken for 1 epoch: 14.763630628585815 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0992 Accuracy 0.1904\n",
            "Epoch 23 Batch 50 Loss 0.0897 Accuracy 0.1704\n",
            "Epoch 23 Loss 0.0912 Accuracy 0.1699\n",
            "Time taken for 1 epoch: 14.615723133087158 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0795 Accuracy 0.1714\n",
            "Epoch 24 Batch 50 Loss 0.0817 Accuracy 0.1717\n",
            "Epoch 24 Loss 0.0842 Accuracy 0.1710\n",
            "Time taken for 1 epoch: 14.668904781341553 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0791 Accuracy 0.1782\n",
            "Epoch 25 Batch 50 Loss 0.0775 Accuracy 0.1733\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-9\n",
            "Epoch 25 Loss 0.0775 Accuracy 0.1724\n",
            "Time taken for 1 epoch: 14.955058097839355 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0553 Accuracy 0.1777\n",
            "Epoch 26 Batch 50 Loss 0.0662 Accuracy 0.1744\n",
            "Epoch 26 Loss 0.0687 Accuracy 0.1741\n",
            "Time taken for 1 epoch: 14.635071516036987 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0593 Accuracy 0.1855\n",
            "Epoch 27 Batch 50 Loss 0.0675 Accuracy 0.1745\n",
            "Epoch 27 Loss 0.0687 Accuracy 0.1741\n",
            "Time taken for 1 epoch: 14.62974214553833 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0539 Accuracy 0.1777\n",
            "Epoch 28 Batch 50 Loss 0.0623 Accuracy 0.1751\n",
            "Epoch 28 Loss 0.0647 Accuracy 0.1746\n",
            "Time taken for 1 epoch: 14.705445766448975 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0692 Accuracy 0.1782\n",
            "Epoch 29 Batch 50 Loss 0.0595 Accuracy 0.1759\n",
            "Epoch 29 Loss 0.0607 Accuracy 0.1755\n",
            "Time taken for 1 epoch: 14.567898750305176 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0393 Accuracy 0.1816\n",
            "Epoch 30 Batch 50 Loss 0.0552 Accuracy 0.1762\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-10\n",
            "Epoch 30 Loss 0.0564 Accuracy 0.1759\n",
            "Time taken for 1 epoch: 14.956617832183838 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0685 Accuracy 0.1836\n",
            "Epoch 31 Batch 50 Loss 0.0515 Accuracy 0.1772\n",
            "Epoch 31 Loss 0.0531 Accuracy 0.1767\n",
            "Time taken for 1 epoch: 14.594663858413696 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0278 Accuracy 0.1592\n",
            "Epoch 32 Batch 50 Loss 0.0505 Accuracy 0.1780\n",
            "Epoch 32 Loss 0.0525 Accuracy 0.1770\n",
            "Time taken for 1 epoch: 14.636891603469849 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0449 Accuracy 0.1802\n",
            "Epoch 33 Batch 50 Loss 0.0531 Accuracy 0.1758\n",
            "Epoch 33 Loss 0.0537 Accuracy 0.1763\n",
            "Time taken for 1 epoch: 14.702069520950317 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0316 Accuracy 0.1821\n",
            "Epoch 34 Batch 50 Loss 0.0481 Accuracy 0.1760\n",
            "Epoch 34 Loss 0.0498 Accuracy 0.1776\n",
            "Time taken for 1 epoch: 14.721853256225586 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0421 Accuracy 0.1772\n",
            "Epoch 35 Batch 50 Loss 0.0482 Accuracy 0.1783\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-11\n",
            "Epoch 35 Loss 0.0487 Accuracy 0.1776\n",
            "Time taken for 1 epoch: 14.926822900772095 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0363 Accuracy 0.1963\n",
            "Epoch 36 Batch 50 Loss 0.0447 Accuracy 0.1783\n",
            "Epoch 36 Loss 0.0458 Accuracy 0.1781\n",
            "Time taken for 1 epoch: 14.694040775299072 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0350 Accuracy 0.1821\n",
            "Epoch 37 Batch 50 Loss 0.0442 Accuracy 0.1786\n",
            "Epoch 37 Loss 0.0458 Accuracy 0.1781\n",
            "Time taken for 1 epoch: 14.594692468643188 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0497 Accuracy 0.2036\n",
            "Epoch 38 Batch 50 Loss 0.0418 Accuracy 0.1786\n",
            "Epoch 38 Loss 0.0440 Accuracy 0.1785\n",
            "Time taken for 1 epoch: 14.685880899429321 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0505 Accuracy 0.1797\n",
            "Epoch 39 Batch 50 Loss 0.0438 Accuracy 0.1782\n",
            "Epoch 39 Loss 0.0450 Accuracy 0.1782\n",
            "Time taken for 1 epoch: 14.593235731124878 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0399 Accuracy 0.1724\n",
            "Epoch 40 Batch 50 Loss 0.0433 Accuracy 0.1792\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-12\n",
            "Epoch 40 Loss 0.0435 Accuracy 0.1790\n",
            "Time taken for 1 epoch: 15.1219162940979 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0377 Accuracy 0.1802\n",
            "Epoch 41 Batch 50 Loss 0.0409 Accuracy 0.1796\n",
            "Epoch 41 Loss 0.0423 Accuracy 0.1790\n",
            "Time taken for 1 epoch: 14.560949087142944 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0510 Accuracy 0.1831\n",
            "Epoch 42 Batch 50 Loss 0.0437 Accuracy 0.1767\n",
            "Epoch 42 Loss 0.0441 Accuracy 0.1781\n",
            "Time taken for 1 epoch: 14.673126697540283 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0325 Accuracy 0.1753\n",
            "Epoch 43 Batch 50 Loss 0.0427 Accuracy 0.1791\n",
            "Epoch 43 Loss 0.0435 Accuracy 0.1778\n",
            "Time taken for 1 epoch: 14.585078716278076 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0429 Accuracy 0.1777\n",
            "Epoch 44 Batch 50 Loss 0.0389 Accuracy 0.1790\n",
            "Epoch 44 Loss 0.0397 Accuracy 0.1792\n",
            "Time taken for 1 epoch: 14.673969030380249 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0274 Accuracy 0.1582\n",
            "Epoch 45 Batch 50 Loss 0.0414 Accuracy 0.1782\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-13\n",
            "Epoch 45 Loss 0.0419 Accuracy 0.1785\n",
            "Time taken for 1 epoch: 14.937322616577148 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0379 Accuracy 0.1841\n",
            "Epoch 46 Batch 50 Loss 0.0396 Accuracy 0.1783\n",
            "Epoch 46 Loss 0.0399 Accuracy 0.1790\n",
            "Time taken for 1 epoch: 14.664791584014893 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0225 Accuracy 0.1748\n",
            "Epoch 47 Batch 50 Loss 0.0351 Accuracy 0.1813\n",
            "Epoch 47 Loss 0.0366 Accuracy 0.1804\n",
            "Time taken for 1 epoch: 14.59814190864563 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0375 Accuracy 0.1904\n",
            "Epoch 48 Batch 50 Loss 0.0346 Accuracy 0.1805\n",
            "Epoch 48 Loss 0.0352 Accuracy 0.1807\n",
            "Time taken for 1 epoch: 14.637367963790894 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0246 Accuracy 0.2012\n",
            "Epoch 49 Batch 50 Loss 0.0328 Accuracy 0.1832\n",
            "Epoch 49 Loss 0.0326 Accuracy 0.1816\n",
            "Time taken for 1 epoch: 14.670219421386719 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0236 Accuracy 0.2007\n",
            "Epoch 50 Batch 50 Loss 0.0297 Accuracy 0.1822\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-14\n",
            "Epoch 50 Loss 0.0312 Accuracy 0.1818\n",
            "Time taken for 1 epoch: 14.97568154335022 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0223 Accuracy 0.1865\n",
            "Epoch 51 Batch 50 Loss 0.0290 Accuracy 0.1811\n",
            "Epoch 51 Loss 0.0296 Accuracy 0.1823\n",
            "Time taken for 1 epoch: 14.612018346786499 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.0267 Accuracy 0.1846\n",
            "Epoch 52 Batch 50 Loss 0.0284 Accuracy 0.1832\n",
            "Epoch 52 Loss 0.0294 Accuracy 0.1818\n",
            "Time taken for 1 epoch: 14.652986764907837 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0256 Accuracy 0.1963\n",
            "Epoch 53 Batch 50 Loss 0.0285 Accuracy 0.1826\n",
            "Epoch 53 Loss 0.0284 Accuracy 0.1829\n",
            "Time taken for 1 epoch: 14.61777114868164 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.0429 Accuracy 0.1787\n",
            "Epoch 54 Batch 50 Loss 0.0259 Accuracy 0.1839\n",
            "Epoch 54 Loss 0.0273 Accuracy 0.1831\n",
            "Time taken for 1 epoch: 14.70105242729187 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0251 Accuracy 0.1792\n",
            "Epoch 55 Batch 50 Loss 0.0264 Accuracy 0.1830\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-15\n",
            "Epoch 55 Loss 0.0265 Accuracy 0.1831\n",
            "Time taken for 1 epoch: 15.029977560043335 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.0266 Accuracy 0.1748\n",
            "Epoch 56 Batch 50 Loss 0.0245 Accuracy 0.1836\n",
            "Epoch 56 Loss 0.0246 Accuracy 0.1834\n",
            "Time taken for 1 epoch: 14.688411235809326 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0321 Accuracy 0.1953\n",
            "Epoch 57 Batch 50 Loss 0.0242 Accuracy 0.1833\n",
            "Epoch 57 Loss 0.0247 Accuracy 0.1835\n",
            "Time taken for 1 epoch: 14.608065843582153 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.0149 Accuracy 0.1719\n",
            "Epoch 58 Batch 50 Loss 0.0225 Accuracy 0.1831\n",
            "Epoch 58 Loss 0.0232 Accuracy 0.1837\n",
            "Time taken for 1 epoch: 14.597529649734497 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0171 Accuracy 0.1758\n",
            "Epoch 59 Batch 50 Loss 0.0224 Accuracy 0.1839\n",
            "Epoch 59 Loss 0.0231 Accuracy 0.1836\n",
            "Time taken for 1 epoch: 14.6373929977417 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.0160 Accuracy 0.1772\n",
            "Epoch 60 Batch 50 Loss 0.0212 Accuracy 0.1853\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-16\n",
            "Epoch 60 Loss 0.0214 Accuracy 0.1847\n",
            "Time taken for 1 epoch: 14.908572435379028 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0172 Accuracy 0.1875\n",
            "Epoch 61 Batch 50 Loss 0.0206 Accuracy 0.1857\n",
            "Epoch 61 Loss 0.0204 Accuracy 0.1848\n",
            "Time taken for 1 epoch: 14.743673086166382 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.0128 Accuracy 0.1719\n",
            "Epoch 62 Batch 50 Loss 0.0197 Accuracy 0.1861\n",
            "Epoch 62 Loss 0.0198 Accuracy 0.1851\n",
            "Time taken for 1 epoch: 14.660964965820312 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0211 Accuracy 0.1885\n",
            "Epoch 63 Batch 50 Loss 0.0187 Accuracy 0.1857\n",
            "Epoch 63 Loss 0.0188 Accuracy 0.1853\n",
            "Time taken for 1 epoch: 14.611178159713745 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.0132 Accuracy 0.1885\n",
            "Epoch 64 Batch 50 Loss 0.0190 Accuracy 0.1851\n",
            "Epoch 64 Loss 0.0192 Accuracy 0.1848\n",
            "Time taken for 1 epoch: 14.590789794921875 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0152 Accuracy 0.1851\n",
            "Epoch 65 Batch 50 Loss 0.0184 Accuracy 0.1858\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-17\n",
            "Epoch 65 Loss 0.0185 Accuracy 0.1852\n",
            "Time taken for 1 epoch: 14.888500690460205 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.0163 Accuracy 0.1694\n",
            "Epoch 66 Batch 50 Loss 0.0183 Accuracy 0.1857\n",
            "Epoch 66 Loss 0.0184 Accuracy 0.1853\n",
            "Time taken for 1 epoch: 14.604310989379883 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0120 Accuracy 0.2085\n",
            "Epoch 67 Batch 50 Loss 0.0173 Accuracy 0.1860\n",
            "Epoch 67 Loss 0.0173 Accuracy 0.1856\n",
            "Time taken for 1 epoch: 14.610774278640747 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.0117 Accuracy 0.1572\n",
            "Epoch 68 Batch 50 Loss 0.0176 Accuracy 0.1844\n",
            "Epoch 68 Loss 0.0181 Accuracy 0.1854\n",
            "Time taken for 1 epoch: 14.670544385910034 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0079 Accuracy 0.1807\n",
            "Epoch 69 Batch 50 Loss 0.0151 Accuracy 0.1855\n",
            "Epoch 69 Loss 0.0155 Accuracy 0.1864\n",
            "Time taken for 1 epoch: 14.508423805236816 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.0099 Accuracy 0.1992\n",
            "Epoch 70 Batch 50 Loss 0.0139 Accuracy 0.1877\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-18\n",
            "Epoch 70 Loss 0.0150 Accuracy 0.1864\n",
            "Time taken for 1 epoch: 14.913539409637451 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0143 Accuracy 0.1968\n",
            "Epoch 71 Batch 50 Loss 0.0147 Accuracy 0.1864\n",
            "Epoch 71 Loss 0.0149 Accuracy 0.1864\n",
            "Time taken for 1 epoch: 14.618730545043945 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.0104 Accuracy 0.1807\n",
            "Epoch 72 Batch 50 Loss 0.0149 Accuracy 0.1866\n",
            "Epoch 72 Loss 0.0151 Accuracy 0.1863\n",
            "Time taken for 1 epoch: 14.68417501449585 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0302 Accuracy 0.1973\n",
            "Epoch 73 Batch 50 Loss 0.0157 Accuracy 0.1871\n",
            "Epoch 73 Loss 0.0156 Accuracy 0.1860\n",
            "Time taken for 1 epoch: 14.613812685012817 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.0081 Accuracy 0.1885\n",
            "Epoch 74 Batch 50 Loss 0.0141 Accuracy 0.1875\n",
            "Epoch 74 Loss 0.0142 Accuracy 0.1867\n",
            "Time taken for 1 epoch: 14.590964078903198 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0092 Accuracy 0.1782\n",
            "Epoch 75 Batch 50 Loss 0.0135 Accuracy 0.1868\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-19\n",
            "Epoch 75 Loss 0.0134 Accuracy 0.1868\n",
            "Time taken for 1 epoch: 14.898324251174927 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0071 Accuracy 0.1943\n",
            "Epoch 76 Batch 50 Loss 0.0131 Accuracy 0.1861\n",
            "Epoch 76 Loss 0.0135 Accuracy 0.1869\n",
            "Time taken for 1 epoch: 14.629698753356934 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0146 Accuracy 0.1816\n",
            "Epoch 77 Batch 50 Loss 0.0118 Accuracy 0.1871\n",
            "Epoch 77 Loss 0.0122 Accuracy 0.1872\n",
            "Time taken for 1 epoch: 14.65241551399231 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0102 Accuracy 0.1895\n",
            "Epoch 78 Batch 50 Loss 0.0133 Accuracy 0.1864\n",
            "Epoch 78 Loss 0.0135 Accuracy 0.1866\n",
            "Time taken for 1 epoch: 14.722751379013062 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0072 Accuracy 0.1807\n",
            "Epoch 79 Batch 50 Loss 0.0122 Accuracy 0.1861\n",
            "Epoch 79 Loss 0.0127 Accuracy 0.1870\n",
            "Time taken for 1 epoch: 14.506839036941528 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0144 Accuracy 0.1865\n",
            "Epoch 80 Batch 50 Loss 0.0119 Accuracy 0.1870\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-20\n",
            "Epoch 80 Loss 0.0122 Accuracy 0.1869\n",
            "Time taken for 1 epoch: 14.912691354751587 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0161 Accuracy 0.1821\n",
            "Epoch 81 Batch 50 Loss 0.0118 Accuracy 0.1860\n",
            "Epoch 81 Loss 0.0131 Accuracy 0.1869\n",
            "Time taken for 1 epoch: 14.575305461883545 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.0100 Accuracy 0.1831\n",
            "Epoch 82 Batch 50 Loss 0.0116 Accuracy 0.1873\n",
            "Epoch 82 Loss 0.0114 Accuracy 0.1875\n",
            "Time taken for 1 epoch: 14.69671106338501 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0152 Accuracy 0.1719\n",
            "Epoch 83 Batch 50 Loss 0.0116 Accuracy 0.1868\n",
            "Epoch 83 Loss 0.0119 Accuracy 0.1873\n",
            "Time taken for 1 epoch: 14.682379007339478 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.0157 Accuracy 0.1865\n",
            "Epoch 84 Batch 50 Loss 0.0118 Accuracy 0.1873\n",
            "Epoch 84 Loss 0.0126 Accuracy 0.1869\n",
            "Time taken for 1 epoch: 14.493134498596191 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0104 Accuracy 0.1851\n",
            "Epoch 85 Batch 50 Loss 0.0109 Accuracy 0.1870\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-21\n",
            "Epoch 85 Loss 0.0107 Accuracy 0.1875\n",
            "Time taken for 1 epoch: 14.904231071472168 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.0050 Accuracy 0.1885\n",
            "Epoch 86 Batch 50 Loss 0.0089 Accuracy 0.1887\n",
            "Epoch 86 Loss 0.0096 Accuracy 0.1878\n",
            "Time taken for 1 epoch: 14.560055494308472 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0071 Accuracy 0.1802\n",
            "Epoch 87 Batch 50 Loss 0.0097 Accuracy 0.1875\n",
            "Epoch 87 Loss 0.0102 Accuracy 0.1880\n",
            "Time taken for 1 epoch: 14.629055261611938 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0096 Accuracy 0.2041\n",
            "Epoch 88 Batch 50 Loss 0.0100 Accuracy 0.1880\n",
            "Epoch 88 Loss 0.0103 Accuracy 0.1876\n",
            "Time taken for 1 epoch: 14.63600778579712 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0087 Accuracy 0.1821\n",
            "Epoch 89 Batch 50 Loss 0.0104 Accuracy 0.1878\n",
            "Epoch 89 Loss 0.0108 Accuracy 0.1877\n",
            "Time taken for 1 epoch: 14.481289625167847 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.0095 Accuracy 0.1719\n",
            "Epoch 90 Batch 50 Loss 0.0103 Accuracy 0.1872\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-22\n",
            "Epoch 90 Loss 0.0106 Accuracy 0.1873\n",
            "Time taken for 1 epoch: 14.923001289367676 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0075 Accuracy 0.1899\n",
            "Epoch 91 Batch 50 Loss 0.0099 Accuracy 0.1881\n",
            "Epoch 91 Loss 0.0102 Accuracy 0.1877\n",
            "Time taken for 1 epoch: 14.604357957839966 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.0071 Accuracy 0.1665\n",
            "Epoch 92 Batch 50 Loss 0.0085 Accuracy 0.1881\n",
            "Epoch 92 Loss 0.0092 Accuracy 0.1881\n",
            "Time taken for 1 epoch: 14.664509296417236 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0049 Accuracy 0.1963\n",
            "Epoch 93 Batch 50 Loss 0.0088 Accuracy 0.1882\n",
            "Epoch 93 Loss 0.0089 Accuracy 0.1881\n",
            "Time taken for 1 epoch: 14.586017847061157 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.0066 Accuracy 0.1890\n",
            "Epoch 94 Batch 50 Loss 0.0092 Accuracy 0.1889\n",
            "Epoch 94 Loss 0.0090 Accuracy 0.1882\n",
            "Time taken for 1 epoch: 14.556944847106934 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0067 Accuracy 0.1729\n",
            "Epoch 95 Batch 50 Loss 0.0079 Accuracy 0.1886\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-23\n",
            "Epoch 95 Loss 0.0083 Accuracy 0.1882\n",
            "Time taken for 1 epoch: 14.849200010299683 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.0053 Accuracy 0.1821\n",
            "Epoch 96 Batch 50 Loss 0.0085 Accuracy 0.1892\n",
            "Epoch 96 Loss 0.0086 Accuracy 0.1880\n",
            "Time taken for 1 epoch: 14.685848474502563 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0069 Accuracy 0.1846\n",
            "Epoch 97 Batch 50 Loss 0.0080 Accuracy 0.1877\n",
            "Epoch 97 Loss 0.0085 Accuracy 0.1882\n",
            "Time taken for 1 epoch: 14.610536098480225 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.0105 Accuracy 0.2095\n",
            "Epoch 98 Batch 50 Loss 0.0102 Accuracy 0.1873\n",
            "Epoch 98 Loss 0.0101 Accuracy 0.1875\n",
            "Time taken for 1 epoch: 14.585072755813599 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0049 Accuracy 0.1880\n",
            "Epoch 99 Batch 50 Loss 0.0089 Accuracy 0.1879\n",
            "Epoch 99 Loss 0.0089 Accuracy 0.1881\n",
            "Time taken for 1 epoch: 14.63851261138916 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.0100 Accuracy 0.1919\n",
            "Epoch 100 Batch 50 Loss 0.0080 Accuracy 0.1883\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-24\n",
            "Epoch 100 Loss 0.0078 Accuracy 0.1884\n",
            "Time taken for 1 epoch: 14.921771764755249 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqYUbDnvJlLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    start_token = [len(input_tokenizer.word_index)]\n",
        "    end_token = [len(input_tokenizer.word_index) + 1]\n",
        "\n",
        "    inp_sentence = [input_tokenizer.word_index[word] for word in add_token(preprocess_sentence(sentence,remove_stopwords=True)).split()]\n",
        "    \n",
        "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "\n",
        "    # 因为目标是英语，输入 transformer 的第一个词应该是\n",
        "    # 英语的开始标记。\n",
        "    decoder_input = [len(output_tokenizer.word_index)]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(max_length_targ):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # 从 seq_len 维度选择最后一个词\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # 如果 predicted_id 等于结束标记，就返回结果\n",
        "    if predicted_id == output_tokenizer.word_index['<end>']:\n",
        "        return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # 连接 predicted_id 与输出，作为解码器的输入传递到解码器。\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPQguSPmYFI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "    sentence = tokenizer_pt.encode(sentence)\n",
        "\n",
        "    attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head+1)\n",
        "\n",
        "        # 画出注意力权重\n",
        "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "        fontdict = {'fontsize': 10}\n",
        "\n",
        "        ax.set_xticks(range(len(sentence)+2))\n",
        "        ax.set_yticks(range(len(result)))\n",
        "\n",
        "        ax.set_ylim(len(result)-1.5, -0.5)\n",
        "\n",
        "        ax.set_xticklabels(\n",
        "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
        "            fontdict=fontdict, rotation=90)\n",
        "\n",
        "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
        "                            if i < tokenizer_en.vocab_size], \n",
        "                           fontdict=fontdict)\n",
        "\n",
        "        ax.set_xlabel('Head {}'.format(head+1))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac30uyC5YJQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summary(sentence, plot=''):\n",
        "    result, attention_weights = evaluate(sentence)\n",
        "\n",
        "    predicted_sentence = \" \".join([output_tokenizer.index_word[id] for id in result.numpy()])\n",
        "\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted summary: {}'.format(predicted_sentence))\n",
        "\n",
        "    if plot:\n",
        "        plot_attention_weights(attention_weights, sentence, result, plot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oboa359RYLVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = pd.read_csv(\"drive/My Drive/Colab Notebooks/data/Reviews.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62iRsZRhYUQH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6f3633e6-fa37-46bb-a88b-9de4410a5846"
      },
      "source": [
        "# Inspecting some of the reviews\n",
        "i = 100\n",
        "print(\"Review #\",i+1)\n",
        "print(reviews.Summary[i])\n",
        "print(reviews.Text[i])"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review # 101\n",
            "Taste wise it is a 6 star item\n",
            "The mouth says, \"How do I love thee, let me count the ways...\"<br />If you like apple products a must have item.  The only draw back, shipping cost.  These are very heavy.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oqG8kFrYmKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = reviews.Text[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RhYKlpsYZ9d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "238235cb-27fa-4b23-aa3c-c384388f1cfd"
      },
      "source": [
        "summary(sentence)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: The mouth says, \"How do I love thee, let me count the ways...\"<br />If you like apple products a must have item.  The only draw back, shipping cost.  These are very heavy.\n",
            "Predicted summary: rainforest tea\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MbbgeBlYiOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}